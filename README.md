# In progress...
![](https://github.com/Memirlan/ViT-vs-ConvNeXt/blob/main/meme.png)

ViT Base 16 and ConvNeXt Base models are of different architectures types, yet their model and parameter sizes are very similar (~330 MB). Also, ConvNeXt was designed as a CNN model that achieves a transformer level performance.

Here, I want to use pretrained versions of both models and try fine-tuning them on a small classification dataset and compare their performance.

ViTs prefer a large dataset with a lower learning rate training, so it shows a slightly poor performance in comparison.
